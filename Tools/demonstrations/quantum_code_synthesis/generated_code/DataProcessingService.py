"""
Data Processing Service
Generated by Quantum Code Synthesis
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any, Union
import pandas as pd
import numpy as np
from dataclasses import dataclass
from datetime import datetime
import json

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ProcessingResult:
    """Result of data processing operation."""
    success: bool
    record_count: int
    statistics: Dict[str, Any]
    processed_at: datetime
    error_message: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "success": self.success,
            "record_count": self.record_count,
            "statistics": self.statistics,
            "processed_at": self.processed_at.isoformat(),
            "error_message": self.error_message
        }


class DataProcessingService:
    """
    Service for processing and analyzing large datasets.

    This service provides quantum-enhanced data processing capabilities
    with automatic optimization for memory usage and processing speed.
    """

    def __init__(self,
                 chunk_size: int = 10000,
                 max_memory_gb: float = 1.0):
        """
        Initialize the data processing service.

        Args:
            chunk_size: Number of rows to process at once
            max_memory_gb: Maximum memory usage in GB
        """
        self.chunk_size = chunk_size
        self.max_memory_gb = max_memory_gb
        self._processing_stats = {
            "total_files_processed": 0,
            "total_records_processed": 0,
            "average_processing_time": 0.0
        }

    async def process_csv_file(self,
                              file_path: str,
                              operations: List[str] = None) -> ProcessingResult:
        """
        Process a CSV file with specified operations.

        Args:
            file_path: Path to the CSV file
            operations: List of operations to perform

        Returns:
            ProcessingResult with statistics and status
        """
        start_time = datetime.now()

        try:
            # Read CSV in chunks for memory efficiency
            chunks = []
            total_records = 0

            for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):
                processed_chunk = await self._process_chunk(chunk, operations or [])
                chunks.append(processed_chunk)
                total_records += len(processed_chunk)

                # Memory check
                if self._check_memory_usage():
                    logger.warning("Approaching memory limit, processing remaining data...")
                    break

            # Combine results
            if chunks:
                result_df = pd.concat(chunks, ignore_index=True)
                statistics = self._calculate_statistics(result_df)
            else:
                statistics = {}

            # Update processing stats
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_processing_stats(total_records, processing_time)

            return ProcessingResult(
                success=True,
                record_count=total_records,
                statistics=statistics,
                processed_at=datetime.now()
            )

        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            return ProcessingResult(
                success=False,
                record_count=0,
                statistics={},
                processed_at=datetime.now(),
                error_message=str(e)
            )

    async def _process_chunk(self,
                           chunk: pd.DataFrame,
                           operations: List[str]) -> pd.DataFrame:
        """
        Process a single chunk of data.

        Args:
            chunk: DataFrame chunk to process
            operations: Operations to apply

        Returns:
            Processed DataFrame
        """
        processed_chunk = chunk.copy()

        for operation in operations:
            if operation == "clean_nulls":
                processed_chunk = processed_chunk.dropna()
            elif operation == "normalize_numeric":
                numeric_columns = processed_chunk.select_dtypes(include=[np.number]).columns
                for col in numeric_columns:
                    processed_chunk[col] = (processed_chunk[col] - processed_chunk[col].mean()) / processed_chunk[col].std()
            elif operation == "add_timestamps":
                processed_chunk['processed_at'] = datetime.now()

        return processed_chunk

    def _calculate_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate comprehensive statistics for the dataset."""
        stats = {
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "column_types": df.dtypes.astype(str).to_dict()
        }

        # Numeric column statistics
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if not numeric_cols.empty:
            stats["numeric_summary"] = df[numeric_cols].describe().to_dict()

        # Categorical column statistics
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if not categorical_cols.empty:
            cat_stats = {}
            for col in categorical_cols:
                value_counts = df[col].value_counts()
                cat_stats[col] = {
                    "unique_values": len(value_counts),
                    "most_common": value_counts.index[0] if not value_counts.empty else None,
                    "most_common_count": value_counts.iloc[0] if not value_counts.empty else 0
                }
            stats["categorical_summary"] = cat_stats

        return stats

    def _check_memory_usage(self) -> bool:
        """Check if memory usage is approaching the limit."""
        # In a real implementation, this would check actual memory usage
        # For demo purposes, return False
        return False

    def _update_processing_stats(self, record_count: int, processing_time: float):
        """Update internal processing statistics."""
        self._processing_stats["total_files_processed"] += 1
        self._processing_stats["total_records_processed"] += record_count

        # Update rolling average
        current_avg = self._processing_stats["average_processing_time"]
        total_files = self._processing_stats["total_files_processed"]
        self._processing_stats["average_processing_time"] = (
            (current_avg * (total_files - 1)) + processing_time
        ) / total_files

    def get_processing_stats(self) -> Dict[str, Any]:
        """Get current processing statistics."""
        return self._processing_stats.copy()

    async def export_results(self,
                           result: ProcessingResult,
                           output_path: str,
                           format: str = "json") -> bool:
        """
        Export processing results to file.

        Args:
            result: Processing result to export
            output_path: Path to output file
            format: Export format ('json' or 'csv')

        Returns:
            True if export successful, False otherwise
        """
        try:
            if format == "json":
                with open(output_path, 'w') as f:
                    json.dump(result.to_dict(), f, indent=2, default=str)
            elif format == "csv":
                # Export statistics as CSV
                stats_df = pd.DataFrame([result.statistics])
                stats_df.to_csv(output_path, index=False)
            else:
                raise ValueError(f"Unsupported export format: {format}")

            logger.info(f"Results exported to {output_path}")
            return True

        except Exception as e:
            logger.error(f"Error exporting results: {e}")
            return False


# Convenience functions
async def process_file_quick(file_path: str) -> ProcessingResult:
    """
    Quick processing function for simple CSV files.

    Args:
        file_path: Path to CSV file

    Returns:
        Processing result
    """
    service = DataProcessingService()
    return await service.process_csv_file(file_path, ["clean_nulls"])


def create_service(chunk_size: int = 10000) -> DataProcessingService:
    """
    Factory function to create a data processing service.

    Args:
        chunk_size: Chunk size for processing

    Returns:
        Configured DataProcessingService instance
    """
    return DataProcessingService(chunk_size=chunk_size)
